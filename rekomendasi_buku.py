# -*- coding: utf-8 -*-
"""Rekomendasi_buku

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/rekomendasi-buku-51ea54c6-70ec-4f76-bb2c-51084ef4b856.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20241107/auto/storage/goog4_request%26X-Goog-Date%3D20241107T225351Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D8d26b127a3c35eab2cd8fe3cfe1db02fdb0f00a39eafae638f0baabf4ae78d9e04b237505ae8f384ece3656b2b78cee7d38cd40bea2eb840cff0ef5733113cd99d645fa3cdce9870434d2d70ed9ea676f6015c3dcb592b28d2bfac28d3ab2c0c9736e38ac5066e49eba1b0e30f643313cd5fdcf82b27e61d3b92645374cba1b98cbbab8fb3c41032dff386f557640afe0f543c6889c358c7dd5840c34944b7efab8288bb6982f694791d9ce42ea73bd6c4a0ff923200164a76634dd0a443f475c7474e0cb3cbb0cade28e32a37c683c57723872d08938c50b92181debc5572860b86f6f4bba959ed330e74d84e65de2cd74fd0b4d7f287f3a2d5791761228393
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
oscarm524_book_recommendation_path = kagglehub.dataset_download('oscarm524/book-recommendation')

print('Data source import complete.')

"""# Business Understanding

Sebelum pandemi, Sebuah toko buku kecil berkembang pesat di pusat kota bersejarah Mainz. Toko tersebut sangat senang membangun hubungan pribadi dengan setiap kliennya, merekomendasikan buku-buku yang sesuai dengan selera pribadi mereka, dan membantu memperluas palet sastra mereka. Di kotanya dan sekitarnya, ia mengembangkan reputasi yang tangguh dengan basis pelanggan setia yang terhormat, yang menganggapnya lebih sebagai seorang penikmat buku daripada penjual tradisional.

Sayangnya, basis pelanggan setia ini tidak cukup untuk membuat bisnisnya menguntungkan. Jadi, seperti banyak pengecer tradisional, Toko buku juga bergantung pada pelanggan tetap.

Pada awal pandemi, sumber pendapatan ini lenyap. Untuk mempertahankan karyawannya dan menutupi biaya yang terus berlanjut, toko buku harus mencari sumber pendapatan alternatif.

Dengan tekad awal yang besar, ia memutuskan untuk mengembangkan usahanya dengan meluncurkan toko daring, yang ia yakini akan menyelamatkan usaha kesayangannya dari kebangkrutan.

Awalnya, pemilik toko dan karyawannya berusaha sebaik mungkin untuk memberikan rekomendasi yang sesuai untuk setiap produk secara manual. Namun, seiring bertambahnya jumlah produk dan karyawan berupaya untuk tetap berhubungan secara pribadi dengan klien melalui telepon dan email, proses manual ini tidak memungkinkan lagi.

Saat ini, toko tersebut tengah mencari sistem rekomendasi yang andal untuk memberikan rekomendasi yang tepat sasaran ke setiap halaman produk. Solusi ini harus memenuhi standar personalisasi yang tinggi dan hanya memerlukan sedikit dukungan manual untuk penerapannya.

**Problem Statements dan Goals**
* Bagaimana membuat sistem rekomendasi yang andal untuk memberikan rekomendasi yang tepat sasaran ke setiap halaman produk.
* Bagaimana sistem rekomendasi dapat memenuhi standar personalisasi yang tinggi dan hanya memerlukan sedikit dukungan manual untuk penerapannya.

**Membuat Sistem Rekomendasi dengan tujuan atau goals sebagai berikut:**
* Menghasilkan sejumlah rekomendasi buku yang dipersonalisasi untuk pengguna dengan teknik content-based filtering.
* Menghasilkan sejumlah rekomendasi buku yang sesuai dengan preferensi pengguna dan belum pernah dikunjungi sebelumnya dengan teknik collaborative filtering.

# Data Understanding

Dalam data understanding kita akan melihat dan memahami data, data rekomendasi buku terdiri dari 3 file csv yaitu “items.csv”, “transactions.csv”, dan “evalutation.csv”

## Load Data

Menyiapkan library awal yang dibutuhkan
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

"""Data disediakan dalam tiga berkas tersendiri. Satu berkas berisi transaksi **(“transactions.csv”)**, satu berkas berisi data item deskriptif **(“items.csv”)**, dan satu berkas terakhir **(“evaluation.csv”)** berisi templat untuk penyerahan hasil. Berikut ini beberapa hal yang perlu diperhatikan tentang berkas-berkas tersebut:

Setelah berhasil mendownload data selanjutnya adalah melakukan pembuatan direktory dataset untuk memudahkan pemanggilan data, berikut codenya:
"""

# Membaca file CSV dengan delimiter yang benar
transaksi = pd.read_csv('/kaggle/input/book-recommendation/transactions.csv', delimiter='|')
items = pd.read_csv('/kaggle/input/book-recommendation/items.csv', delimiter='|')
evaluation = pd.read_csv('/kaggle/input/book-recommendation/evaluation.csv')

# Rename columns of transaksi DataFrame to reflect actual column names
transaksi.columns = ['sessionID', 'itemID', 'click', 'basket', 'order']

# Melihat total data
print('Jumlah data transaksi: ', len(transaksi.itemID.unique()))
print('Jumlah data item: ', len(items.itemID.unique()))
print('Jumlah data evaluasi: ', len(evaluation.itemID.unique()))

"""Dari hasil diatas dapat diketahui jumlah masing-masing data, selanjutnya kita akan melakukan eksplorasi data.

## Univariate Exploratory Data Analysis

Berikut merupakan keterangan atau penjelasan dari masing-masing data:

* File “items.csv” adalah kumpulan data induk yang berisi fitur deskriptif. Fitur tersebut dapat berupa kategoris atau numerik. Daftar fitur dijelaskan dalam file “features.pdf”. Setiap baris data berisi deskripsi untuk satu item.

* File “transactions.csv” berisi informasi tentang klik, keranjang, dan pesanan selama periode tiga bulan. Setiap baris menampilkan satu transaksi untuk satu item. Semua atribut dijelaskan dalam file “features.pdf”.

* File “evalutation.csv” berisi daftar ID produk. Daftar ini merupakan bagian dari produk dari “items.csv” dan referensi untuk pengajuan.

Untuk melakukan rekomendasi buku variabel **items** dan **transaksi** yang akan digunakan sebagai variabel rekomendasi, sementara variabel **evaluasi** merupakan daftar nama itemID yang nantinya akan digabungkan pada variabel items.

**Melihat data pada transaksi**

Langkah berikut merupakan eksplorasi data transaksi, kita kan melihat isi dari varabel transaksi, berikut hasilnya:
"""

#Melihat isi dan tipe data transaksi
transaksi.info()

#Melihat ada berapa banyak entri yang unik
print('Banyak data: ', len(transaksi.sessionID .unique()))
print('Banyak data: ', len(transaksi.itemID .unique()))
print('Jumlah pesana yang diterima: ', transaksi.order.unique())
print('Jumlah klik: ', transaksi.click.unique())
print('Jumlah keranjang: ', transaksi.basket.unique())

"""Dalam hasil diatas terdapat 5 variabel dalam data transaksi yaitu sessionID, itemID, click,	basket,	dan order. selanjutnya untuk memudahkan dalam menganalisa data kita akan lihat 10 baris dari data awal transaksi."""

#Melihat isi data transaksi
transaksi.head(10)

"""Dari hasil diatas dapat diketahui variabel transaksi berikut penjelasannya:
* sessionID: Sesi pada setiap transaksi
* itemID: ID Buku
* click: buku yang dikunjungi atau dilihat
* basket: merupakan memasukan buku yang dipilih kedalam keranjang belanja
* order: merupakan buku yang terpesan atau dibeli pada setiap sesinya.

Selanjutnya kita melakukan eksplorasi dengan data lainnya.

**Melihat data items**

Data items merupakan daftar jenis buku, pengarang, penerbit dan topik buku, mari kita lihat apa saja yang dapat kita ketahui dari data items.
"""

#Melihat isi dan tipe data items
items.info()

print('Banyak data: ', len(items.itemID .unique()))
print('Banyak data: ', len(items.title .unique()))
print('Banyak data: ', len(items.author .unique()))

"""Dari hasil diatas dapat dilihat untuk data items terdapat 6 variabel atau kolom yaitu:
* itemID: ID Buku
* title: Judul buku
* author: Pengarang atau penulis buku
* publisher: penerbit buku
* main topic: Kategori Tema atau topik buku
* subtopics:

Dalam langkah ini kita dapat lebih mengetahui mulai dari judul buku sampai dengan topik atau tema buku yang disimbolkan dengan huruf kapital seperti A B dll. anda dapat melihat penjelasan simbol main topic data ini secara datail pada link berikut: https://ns.editeur.org/thema/en selanjutnya kita melihat isi data items.
"""

#Melihat isi data items
items.head(10)

"""Jika dilihat pada informasi jumlah pada setiap variabel pada data items terdapat ketidak samaan jumlah data oleh sebab itu kita akan melakukan eksplorasi dengan melihat missing value pada data items."""

#melihat missing value
items.isnull().sum()

"""Dapat dilihat variabel author memiliki missing value sebanyak 3240, publisher 9 dan subtopics 1. sementara saya tidak melakukan perbaikan karena saya ingin melakukan eksplorasi data lain dan penggabungan data.

**Melihat data evaluation**

Seperti yang sudah di informasikan sebelumnya data evalution merupakan jumlah itemID pada data rekomendasi buku.
"""

#Melihat isi dan tipe data evaluation
evaluation.info()

#Mengurutkan itemID pada data evaluation
evaluation = evaluation.sort_values('itemID')

#Melihat isi data evaluation
evaluation

"""Dari hasil diatas data evaluation hanya berisi variabel itemID dengan banyak data 1000, ini berarti bahwa jumlah itemID pada data rekomendasi buku terdapat 1000 data.

## Data Preprocessing

Pertama pada tahap ini menggabungkan data evaluation dengan data items pada variabel itemID, hal ini untuk melihat nama pada masing-masing itemID yang terdapat pada data evaluation. parameter yang digunakan adalah:
* on: Parameter on digunakan untuk menentukan nama kolom atau list kolom yang akan digunakan sebagai kunci penggabungan, dalam kasus ini kita menggunkan variabel "itemID"
* how: Parameter how digunakan untuk menentukan metode penggabungan yang akan digunakan dalam kasus ini kta menggunakan "left"

> Ada empat opsi untuk parameter how:
> * 'inner': Menggabungkan hanya baris-baris yang memiliki nilai yang sama pada kolom kunci di kedua DataFrame. Ini adalah metode default jika parameter how tidak ditentukan.
> * 'left': Menggabungkan semua baris dari DataFrame kiri (df1) dengan baris-baris dari DataFrame kanan (df2) yang memiliki nilai yang sama pada kolom kunci. Jika ada baris di DataFrame kiri yang tidak memiliki nilai yang sama di DataFrame kanan, maka nilai kolom di DataFrame kanan akan diisi dengan NaN.
> * 'right': Menggabungkan semua baris dari DataFrame kanan (df2) dengan baris-baris dari DataFrame kiri (df1) yang memiliki nilai yang sama pada kolom kunci. Jika ada baris di DataFrame kanan yang tidak memiliki nilai yang sama di DataFrame kiri, maka nilai kolom di DataFrame kiri akan diisi dengan NaN.
> * 'outer': Menggabungkan semua baris dari kedua DataFrame. Jika ada baris di salah satu DataFrame yang tidak memiliki nilai yang sama di DataFrame lainnya, maka nilai kolom di DataFrame lainnya akan diisi dengan NaN.
"""

# Menggabungkan data items dengan evaluation
evaluation_all = pd.merge(evaluation, items, on='itemID', how='left')
evaluation_all = evaluation_all.sort_values('itemID')
evaluation_all

"""Dari hasil diatas kita sudah dapat mengetahui urutan daftar nama pada itemID yang berjumlah 1000. Selanjutnya kita akan menggabungkan data evaluation_all dengan data transaksi. tujuan penggabungan ini untuk melihat nama itemID yang dikunjungi pada setiap sesi dalam data transaksi. Parameter yang digunakan adalah:
* on: Parameter on digunakan untuk menentukan nama kolom atau list kolom yang akan digunakan sebagai kunci penggabungan, dalam kasus ini kita menggunkan variabel "itemID"
* how: Parameter how digunakan untuk menentukan metode penggabungan yang akan digunakan dalam kasus ini kta menggunakan "inner" (Menggabungkan hanya baris-baris yang memiliki nilai yang sama pada kolom kunci di kedua DataFrame.)
"""

# Menggabungkan evaluation_all dengan transaksi sesuai  pada sessionID
evaluation_transaksi = pd.merge(evaluation_all, transaksi, on='itemID', how='inner')
evaluation_transaksi

#Melihat isi dari data evaluation_transaksi
evaluation_transaksi.head(10)

"""Sampai langkah ini kita sudah dapat melihat nama buku apa saja yang dilihat pada setiap sesi dalam transaksi, namun ada hal yang sedikit menggangu dalam melihat sesinya yaitu variabel sessionID berada ditengah untuk memudahkan dalam melihat data maka kita akan memindahkan variabel sessionID kedepan atau pada kolom pertama berdampingan dengan itemID."""

#Memindahkan kolom sessionID diawal kolom evalution_transaksi
sessionID = evaluation_transaksi.pop('sessionID')
evaluation_transaksi.insert(0, 'sessionID', sessionID)
evaluation_transaksi.head(50)

"""Dengan hasil diatas kita dapat dengan mudah melihat itemID apa yang dilihat pada setiap sesinya."""

#Melihat jumlah data evaluation_transaksi
evaluation_transaksi.shape

"""Dari hasil tersebut terdapat 7711 baris data dengan 10 kolom dalam data tersebut."""

#Melihat statistik data
evaluation_transaksi.describe()

#mengurutkan data berdasarkan sessionID
evaluation_transaksi = evaluation_transaksi.sort_values('sessionID')
evaluation_transaksi.head(50)

"""Setelah kita melihat data yang akan dijadikan data rekomendasi buku terlebih dahulu kita melihat missing value pada data kita, karena pada langkah diatas kita melihat missing value pada data maka dalam tahap ini akan kita selesaikan dan bersihkan datanya."""

#melihat missing value data evalution_all
evaluation_transaksi.isnull().sum()

"""Terlihat data missing pada 3 variabel yaitu variabel author, publisher dan main topic. karena data bertipe object dan keterbatasan dalam pengetahuan data maka data yang memiliki missing value akan dihapus."""

#Menghapus variabel yang memiliki missing value
evaluation_transaksi = evaluation_transaksi.dropna()
evaluation_transaksi.shape

evaluation_transaksi.isnull().sum()

"""Setelah dihapus data berjumlah 5579 data dengan 10 kolom, dan sudah terlihat juga data yang kita miliki sudah bersih atau tidak ada lagi missing value."""

#Melihat isi data
evaluation_transaksi

#mengurutkan data berdasarkan itemID
evaluation_transaksi = evaluation_transaksi.sort_values('itemID')
evaluation_transaksi

"""Sebelum melangkah lebih jauh kita akan melihat berapa jumlah buku berdasarkan itemID."""

#Mengecek berapa banyak jumlah total buku berdasarkan itemID
len(evaluation_transaksi.itemID.unique())

# Mengecek kategori buku yang unik
evaluation_transaksi.title.unique()

#Melihat jumlah buku pada data
evaluation_transaksi.title.value_counts()

"""Selanjutnya, kita hanya akan menggunakan data unik untuk dimasukkan ke dalam proses pemodelan. Oleh karena itu, kita perlu menghapus data yang duplikat dengan fungsi drop_duplicates(). Dalam hal ini, kita membuang data duplikat pada kolom ‘itemID’. Implementasikan kode berikut."""

# Membuang data duplikat pada variabel
evaluation_transaksi = evaluation_transaksi.drop_duplicates('itemID')
evaluation_transaksi

"""Jika dilihat kolom subtopics tidak terlalu mempengaruhi data karena sudah ada pada main topik maka kita akan menghapus kolom subtopic."""

#menghapus variabel atau kolom subtopics
evaluation_transaksi = evaluation_transaksi.drop(['subtopics'], axis=1)
evaluation_transaksi

"""Setelah data tidak ada lagi yang duplikat dan lain sebagainya selanjutnya kita akan merubah nama pada variabel atau kolom dan mengkonversinya dalam bentuk list."""

# Mengonversi data series ‘itemID’ menjadi dalam bentuk list
buku_id = evaluation_transaksi['itemID'].tolist()

# Mengonversi data series ‘title’ menjadi dalam bentuk list
judul_buku = evaluation_transaksi['title'].tolist()

# Mengonversi data series ‘author’ menjadi dalam bentuk list
penulis_buku = evaluation_transaksi['author'].tolist()

# Mengonversi data series ‘publisher’ menjadi dalam bentuk list
penerbit_buku = evaluation_transaksi['publisher'].tolist()

# Mengonversi data series ‘main topic’ menjadi dalam bentuk list
topik_buku = evaluation_transaksi['main topic'].tolist()

print(len(buku_id))
print(len(judul_buku))
print(len(penulis_buku))
print(len(penerbit_buku))
print(len(topik_buku))

"""Tahap berikutnya, kita akan membuat dictionary untuk menentukan pasangan key-value pada data id, judul_buku, penulis_buku, penerbit_buku dan topik_buku yang telah kita siapkan sebelumnya."""

# Membuat dictionary untuk data buku_id, judul_buku, penulis_buku, penerbit_buku, topik_buku
data_buku = {'id': buku_id, 'judul_buku': judul_buku, 'penulis_buku': penulis_buku, 'penerbit_buku': penerbit_buku, 'topik_buku': topik_buku}
# Membuat dataframe dari dictionary data_buku
df_buku = pd.DataFrame(data_buku)
df_buku

#menyimpan data df_buku
df_buku.to_csv('/kaggle/working/buku_final.csv', index=False)

"""## Model Development dengan Content Based Filtering

Pada tahap inilah Anda mengembangkan sistem rekomendasi dengan teknik content based filtering. Ingatlah, teknik content based filtering akan merekomendasikan item yang mirip dengan item yang disukai pengguna di masa lalu. Pada tahap ini, Anda akan menemukan representasi fitur penting dari setiap kategori masakan dengan tfidf vectorizer dan menghitung tingkat kesamaan dengan cosine similarity. Setelah itu, Anda akan membuat sejumlah rekomendasi restoran untuk pelanggan berdasarkan kesamaan yang telah dihitung sebelumnya.
"""

#memanggil dan melihat data
df_buku.sample(5)

"""### Content Based Filtering dengan 1 Variabel Kunci yaitu Topik Buku

**TF-IDF Vectorizer**

Pada tahap ini, kita akan membangun sistem rekomendasi sederhana berdasarkan Topik buku pada sebuah toko buku. Anda telah belajar mengenai TF-IDF Vectorizer pada modul Sentiment Analysis. Teknik tersebut juga akan digunakan pada sistem rekomendasi untuk menemukan representasi fitur penting dari setiap judul_buku dan penulis.

Sama seperti proyek sentiment analysis, pada proyek ini, kita juga menggunakan fungsi tfidfvectorizer() dari library sklearn. Jalankan kode berikut
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Melakukan perhitungan idf pada data penulis_buku
tf.fit(df_buku['topik_buku'])

# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names_out()

"""Selanjutnya, lakukan fit dan transformasi ke dalam bentuk matriks."""

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(df_buku['topik_buku'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""Perhatikanlah, matriks yang kita miliki berukuran (448, 108). Nilai 448 merupakan ukuran data dan 108 merupakan matrik topik_buku.

Untuk menghasilkan vektor tf-idf dalam bentuk matriks, kita menggunakan fungsi todense(). Jalankan kode berikut.
"""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

"""Selanjutnya, mari kita lihat matriks tf-idf untuk beberapa buku (judul_buku) dan topik buku (topik_buku). Terapkan kode berikut."""

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan topik_buku
# Baris diisi dengan judul_buku
pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=df_buku.judul_buku
).sample(22, axis=1).sample(50, axis=0)

"""Output matriks tf-idf di atas menunjukkan judul buku **Companions** memiliki kategori topik buku **fd**. **Companions**, matriks menunjukan bahwa judul buku tersebut merupakan buku dengan topik **fd**. Hal ini terlihat dari nilai matriks 1.0 pada topik buku **fd**.

Sampai di sini, kita telah berhasil mengidentifikasi representasi fitur penting dari setiap topik buku dengan fungsi tfidfvectorizer. Kita juga telah menghasilkan matriks yang menunjukkan korelasi antara topik buku dengan judul buku. Selanjutnya, kita akan menghitung derajat kesamaan antara satu judul_buku dengan judul_buku lainnya untuk menghasilkan kandidat buku yang akan direkomendasikan.

**Cosine Similarity**

Pada tahap sebelumnya, kita telah berhasil mengidentifikasi korelasi antara judul buku dengan topik buku. Sekarang, kita akan menghitung derajat kesamaan (similarity degree) antar judul buku dengan teknik cosine similarity. Di sini, kita menggunakan fungsi cosine_similarity dari library sklearn.

Jalankan kode berikut.
"""

from sklearn.metrics.pairwise import cosine_similarity

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

"""Pada tahapan ini, kita menghitung cosine similarity dataframe tfidf_matrix yang kita peroleh pada tahapan sebelumnya. Dengan satu baris kode untuk memanggil fungsi cosine similarity dari library sklearn, kita telah berhasil menghitung kesamaan (similarity) antar buku. Kode di atas menghasilkan keluaran berupa matriks kesamaan dalam bentuk array.

Selanjutnya, mari kita lihat matriks kesamaan setiap jdul buku dengan menampilkan nama judul buku dalam 5 sampel kolom (axis = 1) dan 10 sampel baris (axis=0). Jalankan kode berikut.
"""

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa judul_buku
cosine_sim_df = pd.DataFrame(cosine_sim, index=df_buku['judul_buku'], columns=df_buku['judul_buku'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap buku
cosine_sim_df

"""**Mendapatkan Rekomendasi**

Sebelumnya, kita telah memiliki data similarity (kesamaan) antar judul buku. Kini, tibalah saatnya  menghasilkan sejumlah judul buku yang akan direkomendasikan kepada pembaca. Untuk lebih memahami bagaimana cara kerjanya, lihatlah kembali matriks similarity pada tahap sebelumnya. Sebagai gambaran, mari kita ambil satu contoh berikut.

Pengguna X pernah memesan buku The Complete Works Of H.P Lovecraft dan A Song of Ice and Fire 05. A Dance with Dragons. Kemudian, saat pengguna tersebut berencana untuk memesan buku dengan judul lain, sistem akan merekomendasikan sesuai topik yang dipesan sebalumnya yaitu The Complete Works Of H.P Lovecraft, dan Gefangen in den Universen. Nah, rekomendasi kedua buku ini berdasarkan kesamaan yang dihitung dengan cosine similarity pada tahap sebelumnya.

Di sini, kita membuat fungsi buku_recommendations dengan beberapa parameter sebagai berikut:

* Judul_buku : Nama Judul Buku (index kemiripan dataframe).
* Similarity_data : Dataframe mengenai similarity yang telah kita definisikan sebelumnya.
* Items : Nama dan fitur yang digunakan untuk mendefinisikan kemiripan, dalam hal ini adalah ‘judul_buku’ dan ‘topik_buku’.
* k : Banyak rekomendasi yang ingin diberikan.

Sebelum mulai menulis kodenya, ingatlah kembali definisi sistem rekomendasi yang menyatakan bahwa keluaran sistem ini adalah berupa top-N recommendation. Oleh karena itu, kita akan memberikan sejumlah rekomendasi restoran pada pengguna yang diatur dalam parameter k.

Jalankan kode berikut.
"""

def buku_recommendations(judul_buku, similarity_data=cosine_sim_df, items=df_buku[['judul_buku', 'topik_buku']], k=5):
    """
    Rekomendasi buku berdasarkan kemiripan dataframe

    Parameter:
    ---
    judul_buku : tipe data string (str)
                Judul Buku (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan buku sebagai
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---


    Pada index ini, kita mengambil k dengan nilai similarity terbesar
    pada index matrix yang diberikan (i).
    """


    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,judul_buku].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop nama_resto agar nama resto yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(judul_buku, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""Perhatikanlah, dengan menggunakan argpartition, kita mengambil sejumlah nilai k tertinggi dari similarity data (dalam kasus ini: dataframe cosine_sim_df). Kemudian, kita mengambil data dari bobot (tingkat kesamaan) tertinggi ke terendah. Data ini dimasukkan ke dalam variabel closest. Berikutnya, kita perlu menghapus judul_buku yang yang dicari agar tidak muncul dalam daftar rekomendasi. Dalam kasus ini, nanti kita akan mencari judul buku yang mirip dengan **Von der Erde zum Mond**, sehingga kita perlu drop judul_buku **Von der Erde zum Mond** agar tidak muncul dalam daftar rekomendasi yang diberikan nanti.  

Selanjutnya, mari kita terapkan kode di atas untuk menemukan rekomendasi buku yang mirip dengan **Von der Erde zum Mond**. Terapkan kode berikut:
"""

df_buku[df_buku.judul_buku.eq('Von der Erde zum Mond')]

# Mendapatkan rekomendasi buku yang mirip dengan Von der Erde zum Mond
buku_recommendations('Von der Erde zum Mond')

"""**SELAMAT** Kita sudah berhasil membuat rekomendasi buku berdasarkan dari topik buku.

### Content Based Filtering dengan Banyak Variabel Kunci

Pada Tahap ini kita akan mencoba melakukan rekomendasi buku dengan lebih dari satu variabel kunci yaitu variabel penulis_buku, penerbit_buku, dan topik_buku.

Adapun langkah awal adalah dengan menggabungkan varibel tertsebut menjadi satu kolom untuk membuat vektorisasi. vektorisasi yang digunakan masih sama yaitu menggunakan tf-idf. Berikut Langkahnya.
"""

#Menggabungkan kolom judul_buku, penulis_buku, penerbit_buku, dan topik_buku
df_buku['combined_features'] = df_buku['penulis_buku'] + " " + df_buku['penerbit_buku']+" " + df_buku['topik_buku']

#Melihat isi data
df_buku.head(10)

"""Setelah Variabel berhasil digabungkan dengan nama "combined_features" selanjutnya kita akan melakukan vektorisasi data pada variabel "combined_features". Berikut Langkahnya"""

# Inisialisasi TfidfVectorizer
tf_buku = TfidfVectorizer(stop_words='english')

# Melakukan perhitungan idf pada data penulis_buku
tf_buku.fit(df_buku['combined_features'])

# Mapping array dari fitur index integer ke fitur nama
tf_buku.get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix_buku = tf_buku.fit_transform(df_buku['combined_features'])

# Melihat ukuran matrix tfidf
tfidf_matrix_buku.shape

"""Perhatikanlah, matriks yang kita miliki berukuran (448, 1152). Nilai 448 merupakan ukuran data dan 1152 merupakan matrik combined_features.

Untuk menghasilkan vektor tf-idf dalam bentuk matriks, kita menggunakan fungsi todense(). Jalankan kode berikut.
"""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix_buku.todense()

"""Selanjutnya, mari kita lihat matriks tf-idf untuk beberapa buku (judul_buku) dan (combined_features). Terapkan kode berikut."""

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan topik_buku
# Baris diisi dengan judul_buku
pd.DataFrame(
    tfidf_matrix_buku.todense(),
    columns=tf_buku.get_feature_names_out(),
    index=df_buku.judul_buku
).sample(22, axis=1).sample(50, axis=0)

"""Output matriks tf-idf di atas menunjukkan judul buku **Collector** memiliki hubungan dengan **fls** hal ini ditunjukkan dengan nilai matrix sebesar **0.41732**.

Sampai di sini, kita telah berhasil mengidentifikasi representasi fitur penting dari setiap combined_features dengan fungsi tfidfvectorizer. Kita juga telah menghasilkan matriks yang menunjukkan korelasi antara topik buku dengan judul buku. Selanjutnya, kita akan menghitung derajat kesamaan antara satu judul_buku dengan judul_buku lainnya untuk menghasilkan kandidat buku yang akan direkomendasikan.
"""

from sklearn.metrics.pairwise import cosine_similarity

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim_buku = cosine_similarity(tfidf_matrix_buku)
cosine_sim_buku

"""Pada tahapan ini, kita menghitung cosine similarity dataframe tfidf_matrix yang kita peroleh pada tahapan sebelumnya. Dengan satu baris kode untuk memanggil fungsi cosine similarity dari library sklearn, kita telah berhasil menghitung kesamaan (similarity) antar buku. Kode di atas menghasilkan keluaran berupa matriks kesamaan dalam bentuk array.

Selanjutnya, mari kita lihat matriks kesamaan setiap jdul buku dengan menampilkan nama judul buku dalam 5 sampel kolom (axis = 1) dan 10 sampel baris (axis=0). Jalankan kode berikut.
"""

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa judul_buku
cosine_sim_df_buku = pd.DataFrame(cosine_sim_buku, index=df_buku['judul_buku'], columns=df_buku['judul_buku'])
print('Shape:', cosine_sim_df_buku.shape)

# Melihat similarity matrix pada setiap buku
cosine_sim_df_buku

"""Selanjutnya setelah kita melihat kesamaan antar judul buku, maka dapat kita buat Sistem Recomendasinya.

Di sini, kita membuat fungsi buku_recommendations dengan beberapa parameter sebagai berikut:

* Judul_buku : Nama Judul Buku (index kemiripan dataframe).
* Similarity_data : Dataframe mengenai similarity yang telah kita definisikan sebelumnya.
* Items : Nama dan fitur yang digunakan untuk mendefinisikan kemiripan, dalam hal ini adalah ‘judul_buku’ dan ‘topik_buku’.
* k : Banyak rekomendasi yang ingin diberikan.

Sebelum mulai menulis kodenya, ingatlah kembali definisi sistem rekomendasi yang menyatakan bahwa keluaran sistem ini adalah berupa top-N recommendation. Oleh karena itu, kita akan memberikan sejumlah rekomendasi restoran pada pengguna yang diatur dalam parameter k.

Jalankan kode berikut.
"""

def buku_recommendations(judul_buku, similarity_data=cosine_sim_df_buku, items=df_buku[['judul_buku', 'combined_features']], k=10):
    """
    Rekomendasi buku berdasarkan kemiripan dataframe

    Parameter:
    ---
    judul_buku : tipe data string (str)
                Judul Buku (index kemiripan dataframe)
    similarity_data : tipe data pd.DataFrame (object)
                      Kesamaan dataframe, simetrik, dengan buku sebagai
                      indeks dan kolom
    items : tipe data pd.DataFrame (object)
            Mengandung kedua nama dan fitur lainnya yang digunakan untuk mendefinisikan kemiripan
    k : tipe data integer (int)
        Banyaknya jumlah rekomendasi yang diberikan
    ---


    Pada index ini, kita mengambil k dengan nilai similarity terbesar
    pada index matrix yang diberikan (i).
    """


    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,judul_buku].to_numpy().argpartition(
        range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop nama_resto agar nama resto yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(judul_buku, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""Langkah dibawah ini merupakan data dari judul buku "Von der Erde zum Mond"
"""

df_buku[df_buku.judul_buku.eq('Von der Erde zum Mond')]

"""Langkah selanjutnya kita dapat melihat hasil rekomendasi buku yang diberikan dari variabel kunci yang sudah digabungkan dan dapat dilihat hasilnya terdapat perbedaan rekomendasi dari variabel yang hanya memakai 1 variabel kunci"""

# Mendapatkan rekomendasi buku yang mirip dengan Von der Erde zum Mond
buku_recommendations('Von der Erde zum Mond')

"""**Selamat** Kita sudah berhasil membuat sistem rekomendasi dengan banyak variabel kunci.

# Model Development dengan Collaborative Filtering

Sebelumnya kita telah menerapkan teknik content based filtering pada data. Teknik ini merekomendasikan item yang mirip dengan preferensi pengguna di masa lalu. Pada tahap ini, kita akan menerapkan teknik collaborative filtering untuk membuat sistem rekomendasi.

Dalam kasus ini, Data tidak memiliki variabel rating sebagai variabel kunci dari rekomendasi, namun kita bisa menganggap variabel klik, basket, dan order sebagai bentuk interaksi antara pengguna dan item (buku), dan menghitung skor implicit feedback untuk mewakili "ketertarikan" pengguna terhadap buku.

**Implicit feedback** adalah data yang dikumpulkan secara tidak langsung dari tindakan pengguna, seperti riwayat pembelian, klik, atau tampilan halaman, yang menunjukkan preferensi mereka tanpa meminta peringkat atau ulasan eksplisit. Data ini digunakan dalam sistem rekomendasi untuk memahami preferensi pengguna dan memberikan rekomendasi yang relevan.

Ada beberapa cara untuk membangun sistem rekomendasi berbasis implicit feedback:
**Pendekatan 1: Skoring Berdasarkan Interaksi**
Anda bisa membuat skor ketertarikan untuk setiap interaksi, misalnya:

* Klik: 1 poin
* Basket: 2 poin
* Order: 3 poin

Skor ini akan menggambarkan tingkat ketertarikan pengguna pada suatu item. Misalnya, jika pengguna mengklik buku, skor ketertarikannya 1, jika memasukkan ke keranjang jadi 2, dan jika melakukan order, skornya 3.

Selanjtnya mari kita implementasikan teknik berikut, dengan dataset transaksi.

### Data Preparation

Kini Anda memasuki tahap preprocessing. Pada tahap ini, Anda perlu melakukan persiapan data dengan terlebih dahulu menyiapkan library yang dibutuhkan.
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from pathlib import Path
import matplotlib.pyplot as plt

"""### Load Datset

Kita akan memanggil dataset buku_final dan tansactions
"""

# Memanggil dataset
import pandas as pd

df_final = pd.read_csv('/kaggle/working/buku_final.csv')
df = pd.read_csv('/kaggle/input/book-recommendation/transactions.csv', delimiter='|')
print(df.head())  # Tampilkan beberapa baris pertama untuk verifikasi

"""Isi dari data transactions yaitu terdapat 4 vriabel sessionID, itemID, click, basket, order.

Selanjutnya kita akan menggabungkan data yang memiliki itemID yang sama.
"""

# Menggabungkan itemID yang sama dan gabungkan dengan sessionID
df = df.groupby('itemID').agg({'sessionID': 'first', 'click': 'sum', 'basket': 'sum', 'order': 'sum'}).reset_index()
print(df)

#merubah kolom session ke kolom awal
df = df[['sessionID', 'itemID', 'click', 'basket', 'order']]
df.head()

"""Langkah selanjutnya adalah menghitung nilai bobot untuk semua jenis interaksi pada variabel click, basket, dan order. Berikut hasilnya:"""

# Tetapkan bobot untuk setiap jenis interaksi
click_weight = 1
basket_weight = 2
order_weight = 3

# Buat kolom 'interaction_score' berdasarkan bobot
df['score'] = (df['click'] * click_weight +
                           df['basket'] * basket_weight +
                           df['order'] * order_weight)
print(df)

"""Dari hasil penghitungan terdapat kolom baru yaitu variabel atau kolom score dan variabel score ini yang akan kita jadikan variabel kunci pengganti rating.

Langkah selanjutnya melihat statistik data.
"""

#Melihat statistik data
df.describe()

"""Dari hasil diatas dapat dilihat pada kolom score memiliki jumlah score terkecil 1 dan yang terbesar 3655.

Selanjutnya kita dapat melihat 10 score terbaik yang diperoleh dari interaksi data.
"""

# Mengambil rekomendasi terbaik berdasarkan rating
top_recommendations = df.sort_values(by='score', ascending=False).head(10)  # Mengambil 10 item teratas
print(top_recommendations[['itemID', 'score']])

# Mengubah sessionID menjadi list tanpa nilai yang sama
session_ids = df['sessionID'].unique().tolist()
print('list sessionID: ', session_ids)

# Melakukan encoding userID
session_to_session_encoded = {x: i for i, x in enumerate(session_ids)}
print('encoded sessionID : ', session_to_session_encoded)

# Melakukan proses encoding angka ke ke userID
session_encoded_to_session = {i: x for i, x in enumerate(session_ids)}
print('encoded angka ke sessionID: ', session_encoded_to_session)

# Mengubah itemID menjadi list tanpa nilai yang sama
item_ids = df['itemID'].unique().tolist()
print('list itemID: ', item_ids)

# Melakukan encoding itemID
buku_to_buku_encoded = {x: i for i, x in enumerate(item_ids)}
print('encoded itemID : ', buku_to_buku_encoded)

# Melakukan proses encoding angka ke ke itemID
buku_encoded_to_buku = {i: x for i, x in enumerate(item_ids)}
print('encoded angka ke itemID: ', buku_encoded_to_buku)

# Mapping sessionID ke dataframe session
df['session'] = df['sessionID'].map(session_to_session_encoded)

# Mapping itemID ke dataframe buku
df['buku'] = df['itemID'].map(buku_to_buku_encoded)

# Melihat dataframe
df.head()

#Melihat jumlah data
df.shape

#menyimpan data df_buku
df.to_csv('/kaggle/working/bdf_score.csv', index=False)

import numpy as np

# Mendapatkan jumlah session
num_session = len(session_to_session_encoded)
print(num_session)

# Mendapatkan jumlah buku
num_buku = len(buku_encoded_to_buku)
print(num_buku)

# Mengubah interaction_score menjadi nilai float
df['score'] = df['score'].values.astype(np.float32)

# Nilai minimum interaction_score
min_rating = min(df['score'])

# Nilai maksimal interaction_score
max_rating = max(df['score'])

print('Number of Session: {}, Number of Buku: {}, Min interaction_score: {}, Max rating: {}'.format(
    num_session, num_buku, min_rating, max_rating
))

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

# Membuat variabel x untuk mencocokkan data session dan buku menjadi satu value
x = df[['session', 'buku']].values

# Membuat variabel y untuk membuat interaction_score dari hasil
y = df['score'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

"""## Proses Training"""

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_session, num_buku, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_session = num_session
    self.num_buku = num_buku
    self.embedding_size = embedding_size
    self.session_embedding = layers.Embedding( # layer embedding user
        num_session,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.session_bias = layers.Embedding(num_session, 1) # layer embedding user bias
    self.buku_embedding = layers.Embedding( # layer embeddings resto
        num_buku,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.buku_bias = layers.Embedding(num_buku, 1) # layer embedding resto bias

  def call(self, inputs):
    session_vector = self.session_embedding(inputs[:,0]) # memanggil layer embedding 1
    session_bias = self.session_bias(inputs[:, 0]) # memanggil layer embedding 2
    buku_vector = self.buku_embedding(inputs[:, 1]) # memanggil layer embedding 3
    buku_bias = self.buku_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_session_buku = tf.tensordot(session_vector, buku_vector, 2)

    x = dot_session_buku + session_bias + buku_bias

    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_session, num_buku, 50) # inisialisasi model

# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 100,
    validation_data = (x_val, y_val)
)

"""# Evaluasi"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Hasil Evaluasi pada data training cukup baik dengan root_mean_squared_error: 0.0076, walaupun pada data test perbedaan yang cukup jauh yaitu val_root_mean_squared_error: 0.4431. Akan tetapi ini masih dalam kategori baik untuk sebuah hasil dalam melakukan rekomendasi.

**Mendapatkan Rekomendasi**

Selanjutnya kita akan membuat sebuah rekomendasi berdasarkan nilai score dari interaksi yang diberikan oleh customer atau pengguna yang telah mengunjungi toko buku, dari data tersebut sistem akan memberikan rekomendasi pada pengunjung baru. Barikut hasilnya.
"""

import pandas as pd
import numpy as np

# Membaca data buku
df_book = pd.read_csv('/kaggle/working/bdf_score.csv')
buku_df = pd.read_csv('/kaggle/working/buku_final.csv')

# Mengambil sample user
session_id = df_book.sessionID.sample(1).iloc[0]
buku_visited_by_session = df_book[df_book.sessionID == session_id]

# Mendapatkan ID buku yang belum dikunjungi
buku_not_visited = buku_df[~buku_df['id'].isin(buku_visited_by_session.itemID.values)]['id']
buku_not_visited = list(
    set(buku_not_visited)
    .intersection(set(buku_to_buku_encoded.keys()))
)

# Mengonversi buku yang belum dikunjungi menjadi format yang sesuai
buku_not_visited = [[buku_to_buku_encoded.get(x)] for x in buku_not_visited]
session_encoder = session_to_session_encoded.get(session_id)
session_buku_array = np.hstack(
    ([[session_encoder]] * len(buku_not_visited), buku_not_visited)
)

# Menghitung prediksi rating untuk buku yang belum dikunjungi
ratings = model.predict(session_buku_array).flatten()

# Mengambil indeks dari 10 rating tertinggi
top_ratings_indices = ratings.argsort()[-10:][::-1]

# Mengonversi indeks kembali ke ID buku
recommended_buku_ids = [
    buku_encoded_to_buku.get(buku_not_visited[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for session: {}'.format(session_id))
print('===' * 9)
print('Buku with high ratings from session')
print('----' * 8)

# Mengambil 5 buku dengan rating tertinggi dari sesi saat ini
top_buku_session = (
    buku_visited_by_session.sort_values(
        by='score',  # Ganti dengan kolom yang sesuai jika diperlukan
        ascending=False
    )
    .head(5)
    .itemID.values
)

# Mengambil detail buku yang sudah dikunjungi
buku_df_rows = buku_df[buku_df['id'].isin(top_buku_session)]
for row in buku_df_rows.itertuples():
    print(row.judul_buku, ':', row.topik_buku)

print('----' * 8)
print('Top 10 buku recommendation based on scores')
print('----' * 8)

# Mengambil detail buku yang direkomendasikan berdasarkan skor tertinggi
recommended_buku = buku_df[buku_df['id'].isin(recommended_buku_ids)]
for row in recommended_buku.itertuples():
    print(row.judul_buku, ':', row.topik_buku)

"""**Selamat** Kita sudah dapat membuat sistem rekomendasi dengan teknik collaborative Filtering dengan melakukan top 10 buku yang memiliki nilai terbaik, dan buku dengan high score terbaik.

Dari hasil diatas terdapat kekurangan dalam menampilkan buku dengan score terbaik karena nilai score memiliki nilai yang sangat beragam atau bervariasi.
"""